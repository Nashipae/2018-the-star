Facebook said it's been more successful at removing terrorist content from its platform - owed in large part to its advanced AI tools. The social media giant said on Thursday it took down 3 million pieces of terrorist content from its site in the third quarter, down from 9.4 million pieces in the second quarter. It said new machine learning tools have helped it pull down posts that may signal support for ISIS or al-Qaeda faster than before. Facebook said the AI gives each post a score showing how likely it is to violate the firm's counterterrorism policies, which allows its human reviewers to prioritize which posts need attention first. Facebook has also reduced the median time between when a user first reports a terrorist post and when it takes it down. The firm has focused its approach toward removing terrorist content before it's viewed by a wide audience. In the third quarter, that median time was 18 hours, which is a significant decrease from 43 hours in the first quarter and 22 hours in the second quarter. 'In both Q2 and Q3 we found more than 99% of the ISIS and al-Qaeda content ultimately removed ourselves, before it was reported by anyone in our community,' Monika Bickert, Facebook's global head of policy, and Brian Fishman, Facebook's head of counterterrorism policy, wrote in a blog post. The firm says it now uses machine learning to spot posts that may signal support for these terrorist groups. In some cases, Facebook automatically removes posts that are given a 'very high' score. 'We still rely on specialized reviewers to evaluate most posts, and only immediately remove posts when the tool’s confidence level is high enough that its “decision” indicates it will be more accurate than our human reviewers,' Bickert and Fishman explained. What's more, Facebook's algorithms can now understand roughly 19 different languages. Some of the 3 million posts identified were old, but Facebook said it was able to pull 2.3 million brand new posts in the third quarter. The firm says it will continue to work hard to remove terrorist content. '[We] should not view this as a problem that can be “solved” and set aside, even in the most optimistic scenarios,' Bickert and Fishman said. 'We can reduce the presence of terrorism on mainstream social platforms, but eliminating it completely requires addressing the people and organizations that generate this material in the real-world.'